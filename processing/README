Just a bunch of notes how to run this prototype:

 + install libhdfs3 (from continuum-io fork master)
 + download hadoop (3.1.0?), configure hdfs and run it (on default port, 8020)
     + see also: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html
     + see "Pseudo-distributed Operation"
     + in short:
         + create config files (core-site.xml: dfs.replication=1, hfds-site.xml: fs.defaultFS=hdfs://localhost:8020)
         + setup ssh keys (maybe use ssh-agent instead of passwordless keys...)
         + format hfds: $ bin/hdfs namenode -format
         + start hdfs:  $ sbin/start-dfs.sh
 + create a virtualenv with the requirements installed
   (dask, distributed, numpy, dask/hdfs3 from master)
 + with the venv activated:
     + $ dask-scheduler
     + $ PYTHONPATH=. dask-worker --nprocs=1 --nthreads=4 --host=<yourhostname> tcp://<where-your-dask-scheduler-is-running>:8786
     + the dask worker needs to be run from this directory, as it needs to find the code somehow
       (here via PYTHONPATH, later it will become easier)
 + use ingest prototype to load some data
 + execute example.py
